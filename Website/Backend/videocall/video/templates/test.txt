// Define emotion labels
const emotion_dict = {
    0: "Frustrated",
    1: "Confused",
    2: "Confused",
    3: "Excited",
    4: "Neutral",
    5: "Sad",
    6: "Surprised"
};

// Function to initialize webcam and emotion detection
async function init() {
    const videoElement = document.getElementById('video-stream');
    const emotionLabel = document.getElementById('emotion-label');

    try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        videoElement.srcObject = stream;

        const faceDetector = new cv.CascadeClassifier();
        faceDetector.load('path/to/haarcascade_frontalface_default.xml'); // Load your classifier XML

        const emotionModel = await tf.loadLayersModel('path/to/emotion_model/model.json'); // Load your emotion model

        const processFrame = async () => {
            const src = new cv.Mat(videoElement.height, videoElement.width, cv.CV_8UC4);
            const gray = new cv.Mat();
            const faces = new cv.RectVector();

            videoElement.addEventListener('loadeddata', async () => {
                const cap = new cv.VideoCapture(videoElement);
                cap.read(src);

                cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY);
                faceDetector.detectMultiScale(gray, faces);

                for (let i = 0; i < faces.size(); ++i) {
                    const face = faces.get(i);
                    const roiGray = gray.roi(face);
                    const resized = new cv.Mat();
                    cv.resize(roiGray, resized, new cv.Size(48, 48));

                    // Convert the resized image to a tensor
                    const tensor = tf.browser.fromPixels(resized).toFloat().div(255).expandDims(0);

                    // Perform emotion prediction
                    const emotionPrediction = emotionModel.predict(tensor).dataSync();
                    const maxIndex = emotionPrediction.indexOf(Math.max(...emotionPrediction));

                    // Update emotion label
                    emotionLabel.textContent = 'Emotion: ' + emotion_dict[maxIndex];

                    roiGray.delete();
                    resized.delete();
                }

                src.delete();
                gray.delete();
                faces.delete();

                requestAnimationFrame(processFrame);
            });
        };

        processFrame();
    } catch (err) {
        console.error('Error accessing webcam:', err);
    }
}

// Initialize the webcam and emotion detection
window.onload = init;



<!DOCTYPE html>
<html>

<head>
    <style>
        #root {
            width: 100vw;
            height: 100vh;
        }

        #video-frame {
            width: 640px;
            /* Specify the width of the video frame */
            height: 480px;
            /* Specify the height of the video frame */
            margin: 0 auto;
            /* Center the frame horizontally */
            position: relative;
        }

        #emotion-feed {
            width: 100%; /* Make it fill the video frame horizontally */
            height: 100%; /* Make it fill the video frame vertically */
            position: absolute; /* Position it absolutely within the video frame */
            top: 0; /* Align it to the top of the video frame */
            left: 0; /* Align it to the left of the video frame */
        }
    </style>
</head>

<body>
    <div id="root">
        <div id="video-frame">
            <!-- The video feed and emotion detection will happen within this frame -->
            <div id="emotion-feed">
                <!-- Emotion detection webcam feed will be placed here -->
                <img src="{% url 'video_feed' %}" alt="Emotion Detection" width="640" height="480">
            </div>
        </div>
    </div>
    <script src="https://unpkg.com/@zegocloud/zego-uikit-prebuilt/zego-uikit-prebuilt.js"></script>
    <script>
        window.onload = function () {
            function getUrlParams(url) {
                let urlStr = url.split('?')[1];
                const urlSearchParams = new URLSearchParams(urlStr);
                const result = Object.fromEntries(urlSearchParams.entries());
                return result;
            }

            // Generate a Token by calling a method.
            // @param 1: appID
            // @param 2: serverSecret
            // @param 3: Room ID
            // @param 4: User ID
            // @param 5: Username
            const roomID = getUrlParams(window.location.href)['roomID'] || (Math.floor(Math.random() * 10000) + "");
            const userID = Math.floor(Math.random() * 10000) + "";
            const userName = "userName" + userID;
            const appID = 481892060;
            const serverSecret = "cfbe7f824643af32fe3793a33dec4d98";
            const kitToken = ZegoUIKitPrebuilt.generateKitTokenForTest(appID, serverSecret, roomID, userID, userName);

            const zp = ZegoUIKitPrebuilt.create(kitToken);
            zp.joinRoom({
                container: document.querySelector("#root"),
                sharedLinks: [{
                    name: 'Personal link',
                    url: window.location.protocol + '//' + window.location.host + window.location.pathname + '?roomID=' + roomID,
                }],
                scenario: {
                    mode: ZegoUIKitPrebuilt.VideoConference,
                },

                turnOnMicrophoneWhenJoining: false,
                turnOnCameraWhenJoining: true,
                showMyCameraToggleButton: true,
                showMyMicrophoneToggleButton: true,
                showAudioVideoSettingsButton: true,
                showScreenSharingButton: true,
                showTextChat: true,
                showUserList: true,
                maxUsers: 2,
                layout: "Auto",
                showLayoutButton: false,

            });

            
        }
    </script>
</body>



</html>